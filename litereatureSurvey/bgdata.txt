Spark MLlib [22]. MLlib is Sparks scalable machine learning li- brary, which consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filter- ing, and dimensionality reduction, as well as underlying optimiza- tion primitives. It supports writing applications in Java, Scala, or Python and can run on any Hadoop2/YARN cluster with no pre- installation.
Graphs are structures formed by a set of vertices (also called nodes) and a set of edges, which are connections between pairs of vertices. The information extracted from a social network can be easily represented as a graph in which the vertices or nodes rep- resent the users and the edges represent the relationships among them (e.g., a re-tweet of a message or a favourite mark in Twitter). 
The k-core de- composition algorithm is adapted to each framework. The goal of this algorithm is to compute the centrality of each node in a given graph. The results obtained confirm the improvement achieved in terms of execution time for these frameworks based on Hadoop. 
______________________
Text analytics
______________________
A significant portion of the unstructured content collected from social media is text. Text mining techniques can be applied for auto- matic organization, navigation, retrieval, and summary of huge vol- umes of text documents
This concept covers a number of topics and algorithms for text analysis including natural language processing (NLP), information retrieval, data mining, and machine learning 
Information extraction techniques attempt to extract entities and their relationships from texts, allowing for the inference of new meaningful knowledge. These kinds of techniques are the starting point for a number of text mining algorithms. A usual model for representing the content of documents or text is the vector space model. In this model, each document is represented by a vector of frequencies of remaining terms within the document
The term frequency (TF) is a function that relates the number of occurrences of the particular word in the document divided by the number of words in the entire document. Another function that is currently used is the inverse document frequency (IDF); typically, documents are rep- resented as TF-IDF feature vectors. Using this data representation, a document represents a data point in n-dimensional space where n is the size of the corpus vocabulary.
Text data tend to be sparse and high dimensional. A text docu- ment corpus can be represented as a large sparse TF-IDF matrix, and applying dimensionality reduction methods to represent the data in compressed format [63] can be very useful
 Latent semantic indexing [64] is an automatic indexing method that projects both documents and terms into a low-dimensional space that attempts to represent the semantic concepts in the document. This method is based on the singular value decomposition of the term-document matrix, which constructs a low-ranking approximation of the original matrix while preserving the similarity between the documents. Another family of dimension reduction techniques is based on probabilistic topic mod-
els such as latent Dirichlet allocation (LDA) clustering techniques are widely studied in this domain to find hidden information or patterns in text datasets. These techniques can automatically organise a document corpus into clusters or 
sim- ilar groups based on a blind search in an unlabelled data collection, grouping the data with similar properties into clusters without hu- man supervision. Generally, document clustering methods can be mainly categorized into two types [67]: partitioning algorithms that divide a document corpus into a given number of disjoint clusters that are optimal in 
terms of some predefined criteria functions and hierarchical algorithms that group the data points into a hierar- chical tree structure or a dendrogram
parti- tioning algorithms perform better and can also be used to produce hierarchies of higher quality than those returned by the hierarchical ones.
In contrast, the classification problem is one of the main topics in the supervised machine learning literature. Nearly all of the well- known techniques for classification, such as decision trees, associa- tion rules, Bayes methods, nearest neighbour classifiers, SVM classi- fiers, and neural networks, have been extended for automated text categorisation 
 Sentiment classification has been studied exten- sively in the area of opinion mining research, and this problem can be formulated as a classification problem with three classes, positive, negative and neutral. Therefore, most of the existing techniques de- signed for this purpose are based on classifiers
  the online spherical k-means algorithm [74] is a segment-wise approach that was proposed for streaming text clustering. This tech- nique splits the incoming text stream into small segments that can be processed effectively in memory. Then, a set of k-means iterations is applied to each segment in order to cluster them. Moreover, in or- der to consider less important old documents during the clustering process, a decay factor is included.
many studies have been proposed for modelling the information diffusion patterns on social networks. The characteris- tics of the diffusion models are (i) the topological structure of the network (a sub-graph composed of a set of users to whom the infor- mation has been spread) and 
 (ii) temporal dynamics (the evolution of the number of users whom the information has reached over time)
 ____________________________________________
 4.2. Crime analysis
Criminals tend to have repetitive pattern behaviours, and these behaviours are dependent upon situational factors. That is, crime will be concentrated in environments with features that facilitate crim- inal activities [97]. The purpose of crime data analysis is to identify these crime patterns, allowing for detecting and discovering crimes and their relationships with criminals. The knowledge extracted from applying data mining techniques can be very useful in supporting law enforcement agencies.
Communication between citizens and government agencies is mostly through telephones, face-to-face meetings, email, and other digital forms. Most of these communications are saved or transformed into written text and then archived in a digital format, which has led to opportunities for automatic text analysis using NLP techniques to improve the effectiveness of law enforcement [98]. A decision support system that combines the use of NLP techniques, similarity measures, and classification approaches is proposed by Ku and Leroy [99] to automate and facilitate crime analysis. Fil- tering reports and identifying those that are related to the same or similar crimes can provide useful information to analyse crime trends, which allows for apprehending suspects and improving crime prevention
pprehending suspects and improving crime prevention.
Traditional crime data analysis techniques are typically designed to handle one particular type of dataset and often overlook geospa- tial distribution. Geographic knowledge discovery can be used to
discover patterns of criminal behaviour that may help in detecting where, when, and why particular crimes are likely to occur. Based on this concept, Phillips and lee [100] present a crime data analysis tech- nique that allows for discovering co-distribution patterns between large, aggregated and heterogeneous datasets. In this approach, ag- gregated datasets are modelled as graphs that store the geospatial distribution of crime within given regions, and then these graphs are used to discover datasets that show similar geospatial distribution characteristics. The experimental results obtained in this work show that it is possible to discover geospatial co-distribution relationships among crime incidents and socio-economic, socio-demographic and spatial features.
Another analytical technique that is now in high use by law en- forcement agencies to visually identify where crime tends to be high- est is the hotspot mapping. This technique is used to predict where crime may happen, using data from the past to inform 
future ac- tions. Each crime event is represented as a point, allowing for the geographic distribution analysis of these points. A number of map- ping techniques can be used to identify crime hotspots, such as: point mapping, thematic mapping of geographic areas, spatial ellipses, grid thematic mapping, and kernel density estimation (KDE), among oth- ers. Chainey et al. [101] conducted a comparative assessment of these techniques, and the results obtained showed that KDE was the technique that consistently outperformed the others. Moreover, the authors offered a benchmark to compare with the results of other techniques and other crime types, including comparisons between advanced spatial analysis techniques and prediction mapping meth- ods. Another novel approach using spatio-temporally tagged tweets for crime prediction is presented by Gerber [102]. This work shows the use of Twitter, applying a linguistic analysis and statistical topic modelling to automatically identify discussion topics across a city in the United States. The experimental results showed that adding Twit- ter data improved crime prediction performance versus a standard approach based on KDE.
Finally, the use of data mining in fraud detection is very popular, and there are numerous studies on this area. ATM phone scams are one well-known type of fraud. Kirkos et al. [103] analysed the effectiveness of data mining classification techniques (decision trees, neural networks and Bayesian belief networks) for identifying fraud- ulent financial statements, and the experimental results concluded that Bayesian belief networks provided higher accuracy for fraud classification. Another approach to detecting fraud in real-time credit card transactions was presented by Quah and Sriganesh [104]. The system these authors proposed uses a self-organization map to filter and analyse customer behaviour to detect fraud. The main idea is to detect the patterns of the legal cardholder and of the fraudulent transactions through neural network learning and then to develop rules for these two different behaviours. One typical fraud in this area is the ATM phone scams that attempts to transfer a victims money into fraudulent accounts. In order to identify the signs of fraudulent accounts and the patterns of fraudulent transactions, Li et al. [105] ap- plied Bayesian classification and association rules. Detection rules are developed based on the identified signs and applied to the design of a fraudulent account detection system. Table 2 shows a brief summary of all of the applications that were previously mentioned, providing a description of the basic functionalities of each and their main methods.

